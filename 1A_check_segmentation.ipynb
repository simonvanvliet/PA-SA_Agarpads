{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check segmentation made with Ilastik\n",
    "\n",
    "- Loads registered data and probability outputs from Ilastik to visualize\n",
    "- Thresholds probability outputs to produce binary cell masks for the three strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#next two lines make sure that Matplotlib plots are shown properly in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#next line is required for Napari\n",
    "%gui qt\n",
    "\n",
    "#main data analysis packages\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#image viewer\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "#out of memory computation\n",
    "import dask.array as da\n",
    "import dask_image.ndmorph as damorph\n",
    "\n",
    "#path handling\n",
    "import pathlib\n",
    "\n",
    "#image processing packages\n",
    "\n",
    "import skimage.filters as filters\n",
    "from skimage.measure import label, regionprops_table\n",
    "from skimage import morphology\n",
    "\n",
    "#file handling\n",
    "import h5py\n",
    "\n",
    "#dask cash\n",
    "from dask.cache import Cache\n",
    "cache = Cache(2e9)  # Leverage two gigabytes of memory\n",
    "cache.register()    # Turn cache on globally"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to registered file\n",
    "path_regestired_im = pathlib.Path(\"/Volumes/ScientificData/Users/Giulia(botgiu00)/Collaborations/Ashley/2023-04-11-agar-pad-processed/Registration_max_frames/\")\n",
    "\n",
    "#set path to Ilastik output file\n",
    "path_segmented_im = pathlib.Path(\"/Volumes/ScientificData/Users/Giulia(botgiu00)/Collaborations/Ashley/2023-04-11-agar-pad-processed/Probabilities_iteger_8bit_export/\")\n",
    "\n",
    "#set filenames\n",
    "exp_name = \"20230411\"\n",
    "pos_idx = 0\n",
    "\n",
    "#specify the order of the strains in the Ilastik layers\n",
    "idx_SA1 = 0 #SA1 is GFP strain\n",
    "idx_SA2 = 1 #SA2 is RFP strain\n",
    "idx_BG = 2\n",
    "idx_PA = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load single position to check output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create metadata path and file names\n",
    "metadata_path = pathlib.Path(f'./agarpad_{exp_name}.csv')\n",
    "file_name_im = f\"{exp_name}_reg_p{pos_idx:03d}.h5\"\n",
    "file_name_seg = f\"{exp_name}_reg_p{pos_idx:03d}-images_Probabilities.h5\"\n",
    "\n",
    "#load metadata\n",
    "df = pd.read_csv(metadata_path, index_col=0)\n",
    "        \n",
    "#load registered images\n",
    "reg_im_file = h5py.File(path_regestired_im/file_name_im, 'r') #open \n",
    "chunk_size = (1, *reg_im_file['images'].shape[-3:])\n",
    "reg_im = da.from_array(reg_im_file['images'], chunks=chunk_size)\n",
    "\n",
    "#load segmented images\n",
    "seg_im_file = h5py.File(path_segmented_im/file_name_seg, 'r') #open \n",
    "chunk_size = (1, 1,*reg_im_file['images'].shape[-2:])\n",
    "\n",
    "seg_prob = da.from_array(seg_im_file['exported_data'], chunks=chunk_size)\n",
    "\n",
    "#crop to max frame\n",
    "max_frm = int(df.loc[f\"pos{pos_idx:03d}\",\"max_frame\"]) if f\"pos{pos_idx:03d}\" in df.index else reg_im.shape[0]\n",
    "reg_im = reg_im[:max_frm]       \n",
    "seg_prob = seg_prob[:max_frm]       \n",
    "\n",
    "condition = df.loc[f\"pos{pos_idx:03d}\",\"condition\"] if f\"pos{pos_idx:03d}\" in df.index else ''\n",
    "\n",
    "#convert to float if necessary\n",
    "if seg_prob.dtype == 'uint8':\n",
    "    seg_prob = seg_prob.astype('float32')/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Pre-process probability map using filters\n",
    "\n",
    "As a first step probability maps are often processed using a Gaussian blur filter using a small (~1 pixel) size, to ensure that the probability maps are locally smooth. We will use scikit image [`filters.gaussion`](https://scikit-image.org/docs/stable/api/skimage.filters.html) to do this. \n",
    "\n",
    "Afterwards we inspect with Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smooth probabilities\n",
    "sigma = 1\n",
    "\n",
    "p_SA1 = da.map_blocks(filters.gaussian, seg_prob[:,idx_SA1,:,:], sigma, channel_axis=0)\n",
    "p_SA2 = da.map_blocks(filters.gaussian, seg_prob[:,idx_SA2,:,:], sigma, channel_axis=0)\n",
    "p_PA = da.map_blocks(filters.gaussian, seg_prob[:,idx_PA,:,:], sigma, channel_axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check probability image with Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.view_image(reg_im,\n",
    "            channel_axis=1,\n",
    "            name=[\"red\", \"green\", \"phase\"],\n",
    "            colormap=[\"red\", \"green\", \"gray\"])\n",
    "\n",
    "#add probability layer to Napari Viewer\n",
    "prop_layer1 = viewer.add_image(p_SA1, name='pSA1',colormap='gray')\n",
    "prop_layer2 = viewer.add_image(p_SA2, name='pSA2',colormap='gray')\n",
    "prop_layer3 = viewer.add_image(p_PA, name='pPA',colormap='gray')\n",
    "\n",
    "napari.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 2. Convert probability map to semantic segmentation using thresholding\n",
    "\n",
    "Here we try to find a good threshold value to convert probability image to binary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create array with all threshold values to rry, here we use 0,0.01,0.02,...,1\n",
    "thresholds_to_try = np.linspace(0,1,101)\n",
    "\n",
    "#convert the list of 3D stacks to a single 4D stack\n",
    "SA1_stack = da.stack([p_SA1 > t for t in thresholds_to_try], axis=0)\n",
    "SA2_stack = da.stack([p_SA2 > t for t in thresholds_to_try], axis=0)\n",
    "PA_stack = da.stack([p_PA > t for t in thresholds_to_try], axis=0)\n",
    "\n",
    "\n",
    "#add to viewer\n",
    "mask_layer_int1 = viewer.add_image(SA1_stack, name='SA1 mask stack',colormap='gray')\n",
    "mask_layer_int2 = viewer.add_image(SA2_stack, name='SA2 mask stack',colormap='gray')\n",
    "mask_layer_int3 = viewer.add_image(PA_stack, name='PA mask stack',colormap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find threshold values\n",
    "\n",
    "Try to find a threshold value to convert probability image to binary mask.\n",
    "\n",
    "1. Select one of the mask layers (PA/SA1/SA1 mask)\n",
    "2. Change opacity in top left control window\n",
    "3. Change threshold value with bottom most slider (Note: the slider goes from 0-100 this corresponds to a threshold value of 0-1, i.e. divide slider position by 100)\n",
    "4. Write down a threshold value that works well for all images, typically it should be between 0.5-0.7. \n",
    "5. If you cannot find a good threshold value, go back to Ilastik and improve training in the problematic areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Threshold values\n",
    "\n",
    "If Ilastik training is done well, a fixed threshold of about 0.5 should generally work well.\n",
    "\n",
    "Here we use this value, but you can change it below. Note: if the needed threshold deviates from the range 0.5-0.7 it's generally better to go back to Ilastik and train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter a manually chosen value for thresholds\n",
    "tr_SA1 = 0.5\n",
    "tr_SA2 = 0.5\n",
    "tr_PA = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA1_mask = p_SA1 > tr_SA1\n",
    "SA2_mask = p_SA2 > tr_SA2\n",
    "PA_mask = p_PA > tr_PA\n",
    "\n",
    "\n",
    "#add to viewer\n",
    "mask_layer_SA1 = viewer.add_image(SA1_mask, name='SA1 mask',colormap='red',opacity=0.3)\n",
    "mask_layer_SA2 = viewer.add_image(SA2_mask, name='SA2 mask',colormap='green',opacity=0.3)\n",
    "mask_layer_PA = viewer.add_image(PA_mask, name='PA mask',colormap='gray',opacity=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Merge cells in colonies into single mask\n",
    "\n",
    "Here we merge cells that belong to same colony together using a morphological closing operation.\n",
    "\n",
    "You have to adapt the `closing_radius` argument below for each species to make this work well.\n",
    "\n",
    "Check the result in Napari and adapt the `closing_radius` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_radius = 5\n",
    "\n",
    "#create structuring element\n",
    "disk = np.expand_dims(morphology.disk(closing_radius), axis=0)\n",
    "\n",
    "SA1_mask_col = damorph.binary_closing(SA1_mask, disk)\n",
    "SA2_mask_col = damorph.binary_closing(SA2_mask, disk)\n",
    "PA_mask_col = damorph.binary_closing(PA_mask, disk)\n",
    "\n",
    "#add mask to Napari\n",
    "mask_layer_col1 = viewer.add_image(SA1_mask_col, name='SA1 mask colonies',colormap='red',opacity=0.3)\n",
    "mask_layer_col2 = viewer.add_image(SA2_mask_col, name='SA2 mask colonies',colormap='green',opacity=0.3)\n",
    "mask_layer_col3 = viewer.add_image(PA_mask_col, name='PA mask colonies',colormap='gray',opacity=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 4. Clean up semantic segmentation using morphological operations\n",
    "\n",
    "Now we do some basic clean up to remove small objects and fill holes.\n",
    "\n",
    "You might need to adapt the `max_hole_area` and `min_cell_area` below, which are the maximum area of holes that will be filled (in pixels), and the \n",
    "minimum area of objects to keep (in pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask, min_cell_area=20, max_hole_area=40):\n",
    "    mask = da.map_blocks(morphology.remove_small_holes, mask, max_hole_area)\n",
    "    mask = da.map_blocks(morphology.remove_small_objects, mask, min_cell_area)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up cell masks\n",
    "SA1_mask_colcl = clean_mask(SA1_mask_col, min_cell_area=20, max_hole_area=100)\n",
    "SA2_mask_colcl = clean_mask(SA2_mask_col, min_cell_area=20, max_hole_area=100)\n",
    "PA_mask_colcl = clean_mask(PA_mask_col, min_cell_area=20, max_hole_area=100)\n",
    "\n",
    "#add mask to Napari\n",
    "mask_layer_clean1 = viewer.add_image(SA1_mask_colcl, name='SA1 mask cleaned',colormap='red',opacity=0.55)\n",
    "mask_layer_clean2 = viewer.add_image(SA2_mask_colcl, name='SA2 mask cleaned',colormap='green',opacity=0.3)\n",
    "mask_layer_clean3 = viewer.add_image(PA_mask_colcl, name='PA mask cleaned',colormap='gray',opacity=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Convert semantic segmentation into instance segmentation\n",
    "\n",
    "Now we give each colony a unique number (corresponding to a unique color in Napari)\n",
    "\n",
    "Check when colonies start to merge (i.e. when neighboring colors switch to having same color because they touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert binary markers into label markers:\n",
    "SA1_labels = da.map_blocks(label, SA1_mask_colcl)\n",
    "SA2_labels = da.map_blocks(label, SA2_mask_colcl)\n",
    "PA_labels = da.map_blocks(label, PA_mask_colcl)\n",
    "\n",
    "\n",
    "#add markers to Napari\n",
    "nap_marker_labels1 = viewer.add_labels(SA1_labels, name='SA1 Labels')\n",
    "nap_marker_labels2 = viewer.add_labels(SA2_labels, name='SA2 Labels')\n",
    "nap_marker_labels3 = viewer.add_labels(PA_labels, name='PA Labels')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 6. Extract Colony Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA1_labels = SA1_labels.compute()\n",
    "SA2_labels = SA2_labels.compute()\n",
    "PA_labels = PA_labels.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process single frame  \n",
    "def extract_prop_slice(label_im, prop_list, image=None, metadata=None):\n",
    "    '''extract region properties from a single frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    label_im : dask array\n",
    "        the label image\n",
    "    prop_list : list\n",
    "        list of properties to extract\n",
    "    image : dask array\n",
    "        fluorescent image to extract intensity properties from\n",
    "    metadata : dict\n",
    "        dictionary of metadata to add to the table\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        table of region properties\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    label_im = label_im.compute() if isinstance(label_im, da.Array) else label_im\n",
    "    \n",
    "    if image is None:\n",
    "        rp_table = regionprops_table(label_im, properties=prop_list) \n",
    "    else:\n",
    "        #regionprops need color channel to be at end\n",
    "        image = da.moveaxis(image, 0, -1)\n",
    "        rp_table = regionprops_table(label_im, intensity_image=image.compute(), properties=prop_list) \n",
    "    \n",
    "    df = pd.DataFrame(rp_table)\n",
    "    #add the time index\n",
    "    if metadata is not None:\n",
    "        for key, val in metadata.items():\n",
    "            df[key] = val\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify properties to extract \n",
    "prop_list = ['label', \n",
    "            'area', 'centroid', \n",
    "            'axis_major_length', 'axis_minor_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loop over all frames\n",
    "df_SA1 = pd.concat([extract_prop_slice(label, \n",
    "                                       prop_list, \n",
    "                                       metadata = {'frame':t, 'strain':'SA1','condition':condition}) \n",
    "                    for t, label in enumerate(SA1_labels)])\n",
    "\n",
    "df_SA2 = pd.concat([extract_prop_slice(label, \n",
    "                                       prop_list, \n",
    "                                       metadata = {'frame':t, 'strain':'SA2','condition':condition}) \n",
    "                    for t, label in enumerate(SA2_labels)])\n",
    "\n",
    "df_PA = pd.concat([extract_prop_slice(label, \n",
    "                                       prop_list, \n",
    "                                       metadata = {'frame':t, 'strain':'PA','condition':condition}) \n",
    "                    for t, label in enumerate(PA_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_SA1, df_SA2, df_PA]).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Track Colonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_colonies(df, direction='forward'):\n",
    "    '''track colonies in a dataframe of region properties\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        table of region properties\n",
    "    direction : str, optional\n",
    "        forward or backward tracking, by default 'forward'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        table of region properties with colony ids\n",
    "    \n",
    "    '''\n",
    "    #add colony id to dataframe\n",
    "    df = df.copy()\n",
    "    df['colony_id']=-1\n",
    "\n",
    "    #set col_idx for frame 0\n",
    "    frm0 = df['frame']==0\n",
    "    col_idx0 = np.arange(frm0.sum())\n",
    "    df.loc[frm0, 'col_idx'] = col_idx0\n",
    "\n",
    "    #forward tracking\n",
    "    for frm in range(1, df['frame'].max()+1):\n",
    "        col_idx_prev = df[df['frame']==frm-1]['col_idx'].values\n",
    "\n",
    "        #calc distance between centroids\n",
    "        x0 = df[df['frame']==frm-1]['centroid-0'].values\n",
    "        y0 = df[df['frame']==frm-1]['centroid-1'].values\n",
    "\n",
    "        x1 = df[df['frame']==frm]['centroid-0'].values\n",
    "        y1 = df[df['frame']==frm]['centroid-1'].values\n",
    "\n",
    "        if direction == 'forward':\n",
    "            #forward tracking\n",
    "            dx = np.atleast_2d(x0).T - np.atleast_2d(x1) #row is x0, col is x1\n",
    "            dy = np.atleast_2d(y0).T - np.atleast_2d(y1) #row is y0, col is y1\n",
    "        elif direction == 'backward':\n",
    "            #backward tracking\n",
    "            dx = np.atleast_2d(x1).T - np.atleast_2d(x0) #row is x1, col is x0\n",
    "            dy = np.atleast_2d(y1).T - np.atleast_2d(y0) #row is y1, col is y0\n",
    "        \n",
    "        ds = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "        #get column index of minimum distance between each cell in frame 0 and frame 1\n",
    "        idx = np.argmin(ds, axis=1)\n",
    "\n",
    "        #forward tracking\n",
    "        if direction == 'forward':\n",
    "            #init new col_idx\n",
    "            col_idx_new = -1 * np.ones(np.sum(df['frame']==frm))\n",
    "            \n",
    "            #make sure each colony in frame t is only matched to one colony in frame t-1\n",
    "            unique_idx = np.unique(idx)\n",
    "            for id_new in unique_idx:\n",
    "                if np.sum(idx==id_new) == 1: #unique match\n",
    "                    id_old = np.where(idx==id_new)[0][0]\n",
    "                    col_idx_new[id_new] = col_idx_prev[id_old]\n",
    "        elif direction == 'backward':\n",
    "            col_idx_new = col_idx_prev[idx]\n",
    "        \n",
    "        #assign new col_idx\n",
    "        df.loc[df['frame']==frm, 'col_idx'] = col_idx_new\n",
    "        \n",
    "    return df\n",
    "\n",
    "def df_track_to_lin(df):\n",
    "    ''' convert dataframe of tracked colonies to linear array for napari\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        table of region properties with colony ids\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        linear array for napari\n",
    "    \n",
    "    '''\n",
    "    lin_data = np.vstack([\n",
    "        df[\"colony_id\"].to_numpy(dtype=int), \n",
    "        df[\"frame\"].to_numpy(dtype=int), \n",
    "        df[\"centroid-0\"].to_numpy(dtype=int), \n",
    "        df[\"centroid-1\"].to_numpy(dtype=int)]).T\n",
    "\n",
    "    return lin_data[lin_data[:,0]>=0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SA1 = track_colonies(df[df['strain']=='SA1'])\n",
    "df_SA2 = track_colonies(df[df['strain']=='SA2'])\n",
    "df_PA = track_colonies(df[df['strain']=='PA'])\n",
    "\n",
    "df = pd.concat([df_SA1, df_SA2, df_PA]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add tracks to viewer\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_labels(SA1_labels, name='SA1 Labels')\n",
    "viewer.add_tracks(df_track_to_lin(df_SA1), name='Tracks SA1')\n",
    "viewer.add_labels(SA2_labels, name='SA2 Labels')\n",
    "viewer.add_tracks(df_track_to_lin(df_SA2), name='Tracks SA2')\n",
    "viewer.add_labels(PA_labels, name='PA Labels')\n",
    "viewer.add_tracks(df_track_to_lin(df_PA), name='Tracks PA')\n",
    "viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Calculate spatial arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_min_dist(target, source):\n",
    "    \"\"\" calculate distance to closest source point for each target colony\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target : 2D numpy.ndarray or tuple of two 1D numpy.ndarrays\n",
    "        centroids of target colonies\n",
    "    source : 2D numpy.ndarray or tuple of two 1D numpy.ndarrays\n",
    "        centroids of source colonies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        distance to closest source point for each target colony\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #extract x and y coordinates\n",
    "    if isinstance(target, tuple):\n",
    "        target_x = target[0]\n",
    "        target_y = target[1]\n",
    "    elif isinstance(target, np.ndarray):\n",
    "        target_x = target[:,0]\n",
    "        target_y = target[:,1]\n",
    "    else:\n",
    "        raise ValueError('target must be tuple or numpy.ndarray')\n",
    "    \n",
    "    if isinstance(source, tuple):\n",
    "        source_x = source[0]\n",
    "        source_y = source[1]\n",
    "    elif isinstance(source, np.ndarray):\n",
    "        source_x = source[:,0]\n",
    "        source_y = source[:,1]\n",
    "    else:\n",
    "        raise ValueError('source must be tuple or numpy.ndarray')\n",
    "\n",
    "    #distance from target to source\n",
    "    dx = np.atleast_2d(target_x).T - np.atleast_2d(source_x) #row is target col is source\n",
    "    dy = np.atleast_2d(target_y).T - np.atleast_2d(source_y) #row is target col is source\n",
    "    ds = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "    return np.min(ds, axis=1) \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centrod_distance(df):\n",
    "    \"\"\" add distance to closest PA centroid to SA1 and SA2 colonies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        table of region properties with colony ids\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        table of region properties with colony ids and distance to closest PA centroid\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize dataframe\n",
    "    df = df.copy()\n",
    "    df['min_dist_PA_centroid'] = np.nan\n",
    "\n",
    "    for frm in df['frame'].unique():\n",
    "\n",
    "        pos_SA1 = df[(df['frame']==frm) & (df['strain']=='SA1')][['centroid-0', 'centroid-1']].values\n",
    "        pos_SA2 = df[(df['frame']==frm) & (df['strain']=='SA2')][['centroid-0', 'centroid-1']].values\n",
    "        pos_PA = df[(df['frame']==frm) & (df['strain']=='PA')][['centroid-0', 'centroid-1']].values\n",
    "\n",
    "        df.loc[(df['frame']==frm) & (df['strain']=='SA1'), 'min_dist_PA_centroid'] = calc_min_dist(pos_SA1, pos_PA)\n",
    "        df.loc[(df['frame']==frm) & (df['strain']=='SA2'), 'min_dist_PA_centroid'] = calc_min_dist(pos_SA2, pos_PA)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_edge2edge_distance(df,PA_labels,SA1_labels,SA2_labels):\n",
    "    ''' add closets distance between edge of SA and PA colonies \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        table of region properties with colony ids\n",
    "    PA_labels : numpy.ndarray\n",
    "        label image of Pseudomonas aeruginosa colonies\n",
    "    SA1_labels : numpy.ndarray\n",
    "        label image of Staphylococcus aureus 1 colonies\n",
    "    SA2_labels : numpy.ndarray\n",
    "        label image of Staphylococcus aureus 2 colonies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        table of region properties with colony ids and distance to closest edge of PA colony\n",
    "    '''\n",
    "    \n",
    "    #initialize dataframe\n",
    "    df = df.copy()\n",
    "    df['min_dist_PA_edge2edge'] = np.nan\n",
    "    \n",
    "    for frm in df['frame'].unique():\n",
    "\n",
    "        #get pixels of PA colony\n",
    "        x_PA, y_PA = np.nonzero(PA_labels[frm,:])\n",
    "\n",
    "        for i in df.index[(df['frame']==frm)]: #loop through colonies in frame\n",
    "            if df.loc[i, 'strain'] == 'PA': #skip Pseudomonas aeruginosa\n",
    "                continue\n",
    "            if df.loc[i, 'col_idx'] == -1: #skip untracked colonies\n",
    "                continue\n",
    "            \n",
    "            #get pixels of target colony\n",
    "            target_im = SA1_labels[frm,:] if df.loc[i, 'strain'] == 'SA1' else SA2_labels[frm,:] \n",
    "            x_SA, y_SA = np.nonzero(target_im == df.loc[i, 'label'])\n",
    "            \n",
    "            #calculate distance to closest PA colony\n",
    "            dist = calc_min_dist((x_SA, y_SA), (x_PA, y_PA))            \n",
    "            df.loc[i, 'min_dist_PA_edge2edge'] = dist.min()\n",
    "        \n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_centrod_distance(df)\n",
    "df = add_edge2edge_distance(df,PA_labels,SA1_labels,SA2_labels)\n",
    "df.to_csv('colonies.csv')\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "i2i_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c6538b57b9d95695cc8c88818812a736980da96b6d92e389fbfaae31437292d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
